Survey of LLMs - Experience Report

OVERALL CONTRIBUTION: Comprehensive evaluation of LLMs on questions about functional programming (incl. programming assignments, conceptual questions)


In particular, we contribute : 


1) Set-up 
- Answer / Code generation
   * Code generation for HW problems
   * Answer generation for conceptual questions
- Code fixing (fixing syntax errors, type errors, logical errors)


2) Methodology 
- Each LLM (out-of-the-box)for 5 attempts
  (incl. Pre-trained llama model)
  * autograding (only for code generation)
  * manual grading (accroding to different rubrics)
--> Assessment into: Mastery, Proficient, Developing, Beginning, "Garbage"
 --> Label assignments with Basic / Advanced


3) Evaluation / Results:
 Code Generation graded according to grading rubric
 --> Graphs / Tables
   a) Performance overall of a given LLM across all questions
   b) Performance of all the LLM by question (and by Basic / Advance)
(Goal: identify strengths / weaknesses of LLMs)

- Code fixing (fixing syntax errors, type errors, logical errors)
  according to grading rubric
 --> Graphs / Tables
   a) Performance overall of a given LLM across all questions
   b) Performance of all the LLM by question (and by Basic / Advance)
(Goal: identify strengths / weaknesses of LLMs)


4) Lessons learned (1/2 page)
- pixtral, claude-sonnet and gpt-4o are the best in terms code / answer generation
  (deepSeek ?)
- LLMs are better in generating code than conceptual answers
+ Hypothesis: LLMs (we expect them) to better generating code than fixing code
+ Hypothesis: Custom-trained model can be small, local, free, and of comparable performance
- insights for students / instructors regarding whether they can "trust" the output generated by a LLMs.
- comparing this snapshot in time to compare it with later evolutions of LLMs










\section{Introduction}
Large Language Models (LLMs) have made significant strides in various domains, including natural language processing, machine translation, and text summarization. Among these applications, the use of LLMs for program synthesis and generation has been particularly noteworthy. Projects such as OpenAI's Codex \cite{10.1145/3597503.3639219} and DeepMind's AlphaCode \cite{10.1109/ICSE48619.2023.00181} have demonstrated the impressive ability of LLMs to generate code, showcasing their potential in automating programming tasks. Recent studies have extended this application to programming education. These tools can function as valuable educational assets when implemented responsibly and within appropriate contexts.They can effectively assist various programming tasks including code completion \cite{10.1109/COMPSAC57700.2023.00117}, \cite{10.1145/3639474.3640076}, program repair \cite{10.5555/3618408.3618894}, code explanations \cite{Chen2023GPTutorAC}. The output quality of ChatGPT in these tasks can highly depend on the training data and prompt design \cite{10.5555/3618408.3618894}. 

Building on the growing interest in the use of LLMs for programming education, recent research has evaluated their performance in real-world introductory computer science courses, primarily focusing on assessing the capabilities of current models with existing datasets or previous assignments from students \cite{hicke2023aitaintelligentquestionanswerteaching, anishka2024chatgptplayroleteaching}. For example, Codex can offer immediate feedback on students' programming assignments and produce varied code samples to illustrate programming principles \cite{kumar2024impactguidanceinteractionstrategies}, which reveals that Codex can enhance code generation capability for novice users.

% LLM for LRL as a motivation?
However, the related research concerns particularly commonly used programming languages like Java and Python, which leads to a notable gap in research regarding applying LLMs to statically typed functional programming languages, such as OCaml, which are often used in advanced programming courses. Despite its pedagogical and theoretical importance, OCaml remains underrepresented in programming education research and the datasets used to train LLMs. This limitation also reflects a broader challenge: how to use LLMs to support low-resourced programming languages effectively \cite{mora2024syntheticprogrammingelicitationtexttocode, 10.1145/3689735, deng2024assessingcodegenerationintermediate}. This would support programming education and improve tools and learning materials for a language that plays an important role in teaching functional programming.


This study aims to address this gap by evaluating and comparing LLM performance in the context of OCaml programming education. Specifically, we investigate whether LLMs can solve OCaml programming assignments, effectively repair faulty OCaml code, and testing their performance on theoretical questions included on exams. These theoretical questions assess students' understanding of key functional programming concepts, such as recursion, higher-order functions, and type inference, which are central to mastering the language. The findings from this research will provide insights into the capabilities of LLMs in statically typed functional programming and help compare the performance of different models in this area. Also, we aim to explore how well LLMs handle both practical coding tasks and conceptual questions, offering a more comprehensive evaluation of their utility in an educational setting. This will help identify effective ways for students to use LLMs as learning aids and provide educators with guidance on integrating these tools into OCaml coursework. It will also support the design of programming exercises that promote meaningful learning while reducing the risk of misuse. Finally, our evaluations are critical for enabling researchers to make informed decisions about incorporating LLMs into their courses and guiding students on the optimal and responsible use of LLM-powered tools.



Evaluating LLMs poses several challenges. We often lack knowledge about their training data, raising questions about the reproducibility of results. Additionally, the variability in LLM outputs can lead to inconsistent results, complicating the evaluation process. Moreover, equity and inclusion concerns arise, as the cost of evaluating LLMs may be prohibitive for some reviewers or instructors. Despite these challenges, our research aims to shed light on the practical and conceptual aspects of using LLMs for programming education, particularly in statically typed functional programming languages. By addressing these questions, we hope to contribute to the growing body of knowledge on LLMs and their potential to transform programming education.

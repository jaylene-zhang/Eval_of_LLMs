        # Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming
- a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. Test GPT3.5 and GPT 4, and CodeLlama on the document generation and bug fixing scenarios using the an LLM powered chat extension in VSCode IDE.

- Task 1. Bug fixing: assess whether the model fixed the original error, whether it created any new errors, and whether the model-modified code remained syntactically correct after the fix was inserted.
result: GPT-4 tends to slightly outperform GPT-3.5, with Code Llama further behind.

- Task 2. Code generation from NL: From each repository, we select the methods that are: 1) covered by the test cases in the given repository’s test suite, and 2) have a docstring. For each method, we ask an LLM to generate the body of the method given the method’s signature and docstring. We provide the contents of method’s file as context to the LLM, replacing the original method body with a commented line reading ”Your Code Here.”

        # Assessing the Quality of GitHub Copilot’s Code Generation
- Use the HumanEval dataset containing 164 problems to evaluate the quality of code generated by Copilot.
The results suggest that GitHubCopilot was able to generate valid code with a 91.5% success rate. In terms of code correctness, out of 164 problems, 47 (28.7%) were correct. Results show that GitHub Copilot is a promising tool.

- Generate solutions by using a Python file containing the prompt(a combination of the function signature and docstring contained in the function body). Implement the code generation step of the experiment MANUALLY. Also test the model using only Function Names and Parameters Without Prompt. Evaluate based on code correctness, validity, and efficiency.

        # Evaluating Large Language Models Trained on Code
- Introduce that a specialized GPT model, called Codex, could excel at a variety of coding tasks. Generate standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests. Create a sandbox for executing programs.

        # Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT
- compare the performance of these prominent code generation tools in terms of code quality metrics for GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. Results show that ChatGPT generates the most correct code.
- Implement the code generation step of our experiment manually by employing the Visual Studio Code IDE for GitHub Copilot, Given the dynamic characteristic of code generation tools.

        # How Novices Use LLM-Based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment
- Analysis on a data set from 33 learners who learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex.

        # Software Testing with Large Language Models: Survey, Landscape, and Vision (2023 Koli)
- A review of the utilization of LLMs in software testing among 102 relevant papers, and a thorough analysis from both software testing and LLMs perspectives.
- use an online learning environment called Coding Steps.
- analysis shows consistently negative trends between the utilization of the AI Single Prompt approach and positive trends for the utilization of the Hybrid approach.

        # Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming

        # Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming
- a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. Test GPT3.5 and GPT 4, and CodeLlama on the document generation and bug fixing scenarios using the an LLM powered chat extension in VSCode IDE.

- Task 1. Bug fixing: assess whether the model fixed the original error, whether it created any new errors, and whether the model-modified code remained syntactically correct after the fix was inserted.
result: GPT-4 tends to slightly outperform GPT-3.5, with Code Llama further behind.

- Task 2. Code generation from NL: From each repository, we select the methods that are: 1) covered by the test cases in the given repository’s test suite, and 2) have a docstring. For each method, we ask an LLM to generate the body of the method given the method’s signature and docstring. We provide the contents of method’s file as context to the LLM, replacing the original method body with a commented line reading ”Your Code Here.”

        # Assessing the Quality of GitHub Copilot’s Code Generation
- Use the HumanEval dataset containing 164 problems to evaluate the quality of code generated by Copilot.
The results suggest that GitHubCopilot was able to generate valid code with a 91.5% success rate. In terms of code correctness, out of 164 problems, 47 (28.7%) were correctly. Results show that GitHub Copilot is a promising tool.

- Generate solutions by using a Python file containing the prompt(combination of the function signature and docstring contained in the function body). Implement the code generation step of the experiment MANUALLY. Also test the model using only Function Names and Parameters Without Prompt. Evaluate based on code correctness, validity, and efficiency.

        # Evaluating Large Language Models Trained on Code
